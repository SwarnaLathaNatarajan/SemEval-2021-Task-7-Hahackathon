{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at gpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.955144958496094, 'learning_rate': 1.9191919191919194e-05, 'epoch': 0.05060728744939271}\n",
      "{'loss': 0.43979358673095703, 'learning_rate': 1.8181818181818182e-05, 'epoch': 0.10121457489878542}\n",
      "{'loss': 0.346490364074707, 'learning_rate': 1.7171717171717173e-05, 'epoch': 0.15182186234817813}\n",
      "{'loss': 0.34578815460205076, 'learning_rate': 1.616161616161616e-05, 'epoch': 0.20242914979757085}\n",
      "{'loss': 0.3592367935180664, 'learning_rate': 1.5151515151515153e-05, 'epoch': 0.25303643724696356}\n",
      "{'loss': 0.30040239334106444, 'learning_rate': 1.4141414141414143e-05, 'epoch': 0.30364372469635625}\n",
      "{'loss': 0.356728515625, 'learning_rate': 1.3131313131313132e-05, 'epoch': 0.354251012145749}\n",
      "{'loss': 0.4129073333740234, 'learning_rate': 1.2121212121212122e-05, 'epoch': 0.4048582995951417}\n",
      "{'loss': 0.37961257934570314, 'learning_rate': 1.1111111111111113e-05, 'epoch': 0.45546558704453444}\n",
      "{'loss': 0.3658058547973633, 'learning_rate': 1.0101010101010103e-05, 'epoch': 0.5060728744939271}\n",
      "{'loss': 0.3089973640441894, 'learning_rate': 9.090909090909091e-06, 'epoch': 0.5566801619433198}\n",
      "{'loss': 0.2708667945861816, 'learning_rate': 8.08080808080808e-06, 'epoch': 0.6072874493927125}\n",
      "{'loss': 0.30908935546875, 'learning_rate': 7.070707070707071e-06, 'epoch': 0.6578947368421053}\n",
      "{'loss': 0.35254730224609376, 'learning_rate': 6.060606060606061e-06, 'epoch': 0.708502024291498}\n",
      "{'loss': 0.3565912628173828, 'learning_rate': 5.0505050505050515e-06, 'epoch': 0.7591093117408907}\n",
      "{'loss': 0.32153480529785156, 'learning_rate': 4.04040404040404e-06, 'epoch': 0.8097165991902834}\n",
      "{'loss': 0.29649168014526367, 'learning_rate': 3.0303030303030305e-06, 'epoch': 0.8603238866396761}\n",
      "{'loss': 0.3317641448974609, 'learning_rate': 2.02020202020202e-06, 'epoch': 0.9109311740890689}\n",
      "{'loss': 0.3024185371398926, 'learning_rate': 1.01010101010101e-06, 'epoch': 0.9615384615384616}\n",
      "{'loss': 0.2998097610473633, 'learning_rate': 0.0, 'epoch': 1.0121457489878543}\n",
      "{'epoch': 1.0121457489878543}\n"
     ]
    }
   ],
   "source": [
    "import torch, argparse, pandas, numpy\n",
    "\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "\n",
    "\n",
    "from hahadataset import HahaDataset\n",
    "\n",
    "from transformers import GPT2ForSequenceClassification, GPT2Tokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "def metrics_acc(eval_pred):\n",
    "\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = eval_pred.predictions.argmax(-1)\n",
    "    acc = accuracy_score(labels, preds)\n",
    "\n",
    "    return {\"accuracy\": acc}\n",
    "\n",
    "def metrics_rmse(eval_pred):\n",
    "    labels = eval_pred.label_ids\n",
    "    preds = eval_pred.predictions\n",
    "    rmse = mean_squared_error(labels,preds)\n",
    "\n",
    "    rmse = numpy.float64(rmse)\n",
    "\n",
    "    return{\"rmse\": rmse}\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--load_from_checkpoint', type=str)\n",
    "    parser.add_argument('--continue_training', type=str)\n",
    "    parser.add_argument('--output_directory', type=str,default=\"output_dir\")\n",
    "    parser.add_argument('--tokenizer_path', type=str)\n",
    "    parser.add_argument('--max_len', type=int, default=256)\n",
    "    parser.add_argument('--max_steps', type=int, default=500)\n",
    "    parser.add_argument('--batch_size', type=int, default=8)\n",
    "    parser.add_argument('--num_gpus', type=int, default=4)\n",
    "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "    args = parser.parse_args()\n",
    "    task = 'humor_rating'\n",
    "\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "    tokenizer.padding_side = \"left\"\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "\n",
    "    train_dataset = HahaDataset(input_file='./../Data/train_data/train.csv', tokenizer=tokenizer, max_len=256, task=task,\n",
    "                                split='train')\n",
    "    eval_dataset = HahaDataset(input_file='./../Data/train_data/train.csv', tokenizer=tokenizer, max_len=256, task=task,\n",
    "                               split='eval')\n",
    "    test_dataset = HahaDataset(input_file='./../Data/test_data/public_test.csv', tokenizer=tokenizer, max_len=256, task=task,\n",
    "                               split='test')\n",
    "\n",
    "    model = GPT2ForSequenceClassification.from_pretrained('gpt2', num_labels=1)\n",
    "    \n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "    # fix model padding token id\n",
    "    model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "\n",
    "    warmup_steps = int(args.max_steps * .01)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_directory,\n",
    "        max_steps=args.max_steps,\n",
    "        per_device_train_batch_size=args.batch_size,\n",
    "        logging_steps=25,\n",
    "        save_total_limit=1,\n",
    "#         evaluate_during_training=True,\n",
    "        eval_steps=50,\n",
    "        learning_rate=2e-5,\n",
    "        warmup_steps=warmup_steps,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model='eval_rmse',\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=eval_dataset,\n",
    "        compute_metrics=metrics_rmse,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    predictions = trainer.predict(test_dataset)\n",
    "    humor_rating_preds = [pred[0] for pred in predictions.predictions]\n",
    "\n",
    "    output_list = []\n",
    "    for pred in humor_rating_preds:\n",
    "        temp = {}\n",
    "        temp['humor_rating'] = pred\n",
    "        output_list.append(temp)\n",
    "\n",
    "    out_df = pandas.DataFrame(output_list)\n",
    "    out_df.to_csv('submission_humor_rating.csv', index_label='id')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
